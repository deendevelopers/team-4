{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib as pyplot\n",
    "\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "import re\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "quran_df = pd.read_csv('../data/en-sahih.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Surah Name</th>\n",
       "      <th>Surah Number</th>\n",
       "      <th>Verse Number</th>\n",
       "      <th>Verse Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>الفاتحة</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>In the name of Allah, the Entirely Merciful, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>الفاتحة</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[All] praise is [due] to Allah, Lord of the wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>الفاتحة</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>The Entirely Merciful, the Especially Merciful,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>الفاتحة</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Sovereign of the Day of Recompense.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>الفاتحة</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>It is You we worship and You we ask for help.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Surah Name  Surah Number  Verse Number  \\\n",
       "0    الفاتحة             1             1   \n",
       "1    الفاتحة             1             2   \n",
       "2    الفاتحة             1             3   \n",
       "3    الفاتحة             1             4   \n",
       "4    الفاتحة             1             5   \n",
       "\n",
       "                                          Verse Text  \n",
       "0  In the name of Allah, the Entirely Merciful, t...  \n",
       "1  [All] praise is [due] to Allah, Lord of the wo...  \n",
       "2    The Entirely Merciful, the Especially Merciful,  \n",
       "3                Sovereign of the Day of Recompense.  \n",
       "4      It is You we worship and You we ask for help.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quran_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadith_df = pd.read_csv('../data/all_hadiths_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hadith_id</th>\n",
       "      <th>source</th>\n",
       "      <th>chapter_no</th>\n",
       "      <th>hadith_no</th>\n",
       "      <th>chapter</th>\n",
       "      <th>chain_indx</th>\n",
       "      <th>text_ar</th>\n",
       "      <th>text_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Sahih Bukhari</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Revelation - كتاب بدء الوحى</td>\n",
       "      <td>30418, 20005, 11062, 11213, 11042, 3</td>\n",
       "      <td>حدثنا الحميدي عبد الله بن الزبير، قال حدثنا سف...</td>\n",
       "      <td>Narrated 'Umar bin Al-Khattab:          ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Sahih Bukhari</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Revelation - كتاب بدء الوحى</td>\n",
       "      <td>30355, 20001, 11065, 10511, 53</td>\n",
       "      <td>حدثنا عبد الله بن يوسف، قال أخبرنا مالك، عن هش...</td>\n",
       "      <td>Narrated 'Aisha:                        ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Sahih Bukhari</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Revelation - كتاب بدء الوحى</td>\n",
       "      <td>30399, 20023, 11207, 11013, 10511, 53</td>\n",
       "      <td>حدثنا يحيى بن بكير، قال حدثنا الليث، عن عقيل، ...</td>\n",
       "      <td>Narrated 'Aisha:                       (the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Sahih Bukhari</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Revelation - كتاب بدء الوحى</td>\n",
       "      <td>11013, 10567, 34</td>\n",
       "      <td>قال ابن شهاب وأخبرني أبو سلمة بن عبد الرحمن، أ...</td>\n",
       "      <td>Narrated Jabir bin 'Abdullah Al-Ansari while ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Sahih Bukhari</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Revelation - كتاب بدء الوحى</td>\n",
       "      <td>20040, 20469, 11399, 11050, 17</td>\n",
       "      <td>حدثنا موسى بن إسماعيل، قال حدثنا أبو عوانة، قا...</td>\n",
       "      <td>Narrated Said bin Jubair:               ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  hadith_id           source  chapter_no hadith_no  \\\n",
       "0   0          1   Sahih Bukhari            1        1    \n",
       "1   1          2   Sahih Bukhari            1        2    \n",
       "2   2          3   Sahih Bukhari            1        3    \n",
       "3   3          4   Sahih Bukhari            1        4    \n",
       "4   4          5   Sahih Bukhari            1        5    \n",
       "\n",
       "                       chapter                             chain_indx  \\\n",
       "0  Revelation - كتاب بدء الوحى   30418, 20005, 11062, 11213, 11042, 3   \n",
       "1  Revelation - كتاب بدء الوحى         30355, 20001, 11065, 10511, 53   \n",
       "2  Revelation - كتاب بدء الوحى  30399, 20023, 11207, 11013, 10511, 53   \n",
       "3  Revelation - كتاب بدء الوحى                       11013, 10567, 34   \n",
       "4  Revelation - كتاب بدء الوحى         20040, 20469, 11399, 11050, 17   \n",
       "\n",
       "                                             text_ar  \\\n",
       "0  حدثنا الحميدي عبد الله بن الزبير، قال حدثنا سف...   \n",
       "1  حدثنا عبد الله بن يوسف، قال أخبرنا مالك، عن هش...   \n",
       "2  حدثنا يحيى بن بكير، قال حدثنا الليث، عن عقيل، ...   \n",
       "3  قال ابن شهاب وأخبرني أبو سلمة بن عبد الرحمن، أ...   \n",
       "4  حدثنا موسى بن إسماعيل، قال حدثنا أبو عوانة، قا...   \n",
       "\n",
       "                                             text_en  \n",
       "0        Narrated 'Umar bin Al-Khattab:          ...  \n",
       "1        Narrated 'Aisha:                        ...  \n",
       "2   Narrated 'Aisha:                       (the m...  \n",
       "3   Narrated Jabir bin 'Abdullah Al-Ansari while ...  \n",
       "4        Narrated Said bin Jubair:               ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hadith_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = quran_df['Verse Text'].append(hadith_df['text_en'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39824"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(text):\n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    return text\n",
    "\n",
    "def make_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_funcs = [make_lower_case, \n",
    "                  remove_stop_words, \n",
    "                  remove_punctuation, \n",
    "                  stem_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_documents = []\n",
    "for document in documents:\n",
    "    clean_doc = document\n",
    "    for f in cleaning_funcs:\n",
    "        clean_doc = f(clean_doc)\n",
    "    \n",
    "    cleaned_documents.append(clean_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39824, 17854)\n"
     ]
    }
   ],
   "source": [
    "#Fit TFIDF \n",
    "#Learn vocabulary and tfidf from all style_ids.\n",
    "tf = TfidfVectorizer(analyzer='word', \n",
    "                     min_df=10,\n",
    "                     ngram_range=(1, 2),\n",
    "                     #max_features=1000,\n",
    "                     stop_words='english')\n",
    "tf.fit(cleaned_documents)\n",
    "\n",
    "#Transform style_id products to document-term matrix.\n",
    "tfidf_matrix = tf.transform(cleaned_documents)\n",
    "pickle.dump(tf, open(\"../models/tfidf_model.pkl\", \"wb\"))\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39824, 500)\n"
     ]
    }
   ],
   "source": [
    "# Compress with SVD\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=500)\n",
    "latent_matrix = svd.fit_transform(tfidf_matrix)\n",
    "pickle.dump(svd, open(\"../models/svd_model.pkl\", \"wb\"))\n",
    "\n",
    "print(latent_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39824, 50)\n"
     ]
    }
   ],
   "source": [
    "n = 50 #pick components\n",
    "\n",
    "# a\n",
    "# doc_labels = df.title\n",
    "svd_feature_matrix = pd.DataFrame(latent_matrix[:,0:n])\n",
    "print(svd_feature_matrix.shape)\n",
    "svd_feature_matrix.head()\n",
    "\n",
    "pickle.dump(svd_feature_matrix, open(\"../models/lsa_embeddings.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = []\n",
    "for document in cleaned_documents:\n",
    "    split_docs.append(document.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_documents = [gensim.models.doc2vec.TaggedDocument(doc, [i]) \n",
    "                       for i, doc in enumerate(split_docs)]\n",
    "\n",
    "model = Doc2Vec(vector_size=50,\n",
    "                min_count=5,\n",
    "                epochs=200, \n",
    "                seed=0, \n",
    "                window=3, \n",
    "                dm=1)\n",
    "\n",
    "model.build_vocab(formatted_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(formatted_documents, \n",
    "            total_examples=model.corpus_count, \n",
    "            epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uwaisiqbal/miniconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "fname = get_tmpfile(\"../models/doc2vec_model\")\n",
    "model.save(\"../models/doc2vec_model\")\n",
    "model = Doc2Vec.load(\"../models/doc2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.395602  , -1.0176108 ,  0.04891639, -0.23413673, -1.1014264 ,\n",
       "        0.25590667,  0.543723  ,  0.34218332, -1.2481732 ,  0.9248172 ,\n",
       "       -0.12828165, -1.225692  , -0.03927625, -0.1434317 ,  0.36229712,\n",
       "       -0.3615093 ,  0.19589567,  0.5047545 ,  0.43324736, -0.46560958,\n",
       "       -0.14506127,  0.03595288, -0.2649576 , -1.3081894 ,  0.7348324 ,\n",
       "        1.2057209 , -0.05394661, -0.44939655,  0.02879823, -0.8890898 ,\n",
       "        1.5298823 , -1.2370139 , -0.0226839 , -1.7492684 ,  0.67260015,\n",
       "        0.35627657,  0.8090528 ,  0.15275992,  0.31522354,  0.23707627,\n",
       "        0.41513297,  0.03648197,  0.33456528,  0.20344214,  0.65324324,\n",
       "        0.46326482,  0.8589235 ,  0.37457705,  0.70699465, -0.69354427],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = model.infer_vector(doc_words=[\"this\", \"is\", \"a\", \"test\"], epochs=50)\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39824, 50)\n"
     ]
    }
   ],
   "source": [
    "doctovec_feature_matrix = pd.DataFrame(model.docvecs.vectors_docs)\n",
    "print(doctovec_feature_matrix.shape)\n",
    "\n",
    "doctovec_feature_matrix.head(3)\n",
    "pickle.dump(doctovec_feature_matrix, open(\"../models/doctovec_embeddings.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
